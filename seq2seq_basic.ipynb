{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA DOWNLOAD\n",
    "To start this project, you need an addition data file.\n",
    "\n",
    "First you should download data from:\n",
    "https://pan.baidu.com/s/1KFui9zZKjRqzFkCJH5nenw\n",
    "    \n",
    "Then unzip it,create a data dir, and put it in data directory\n",
    "\n",
    "## After you do all that , the following file should be found:\n",
    "\n",
    "```python\n",
    "'data/segmented_train_seg_by_word.txt' \n",
    "```\n",
    "\n",
    "And that means you are good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20000000 data/segmented_train_seg_by_word.txt\r\n"
     ]
    }
   ],
   "source": [
    "# segmented_train_seg_by_word.txt 内容sample\n",
    "# couple of what ? who ' s buddy ?\n",
    "#一对 什么 ？ 哥们儿 是 谁 ？ \n",
    "\n",
    "! wc -l data/segmented_train_seg_by_word.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ProgressBar\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100.00 % [==================================================>] 10000000/10000000 \t used:61s eta:0 s"
     ]
    }
   ],
   "source": [
    "enline = None\n",
    "chline = None\n",
    "\n",
    "sentlength = 5\n",
    "\n",
    "enlines = []\n",
    "chlines = []\n",
    "pb = ProgressBar(worksum=10000000)\n",
    "pb.startjob()\n",
    "num = 0\n",
    "# 中文， 英文 分类， 只留下长度 <=5的句子\n",
    "with open('data/segmented_train_seg_by_word.txt') as fhdl:\n",
    "    for line in fhdl:\n",
    "        num += 1\n",
    "        if num % 2 == 1:\n",
    "            enline = line\n",
    "            continue\n",
    "        else:\n",
    "            chline = line\n",
    "        \n",
    "        enlinesp = [i.lower() for i in enline.strip(\"\\n\").split()]\n",
    "        # text_classify_basic  里chlinesp是split成每个单词，这里没有，而是split成每个字\n",
    "        # chlinesp sample: ['一', '对', '五', '百', '诶', '。']\n",
    "        chlinesp = [i for i in chline.strip(\"\\n\").replace(' ','')]\n",
    "        if len(enlinesp) <= sentlength and len(chlinesp) <= sentlength:\n",
    "            enlines.append(enlinesp)\n",
    "            chlines.append(chlinesp)\n",
    "        if (num // 2) % 1000 == 0:\n",
    "            pb.complete(1000)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['deuces', 'the', 'winner', '.'],\n",
       " ['a', 'couple', 'of', 'what', '?'],\n",
       " ['a', 'pair', 'of', 'wives', '?'],\n",
       " ['husband', 'and', 'wife', '.'],\n",
       " ['couple', '.'],\n",
       " ['nice', 'couple', '.'],\n",
       " ['two', 'lovers', '.'],\n",
       " ['a', 'couple', 'getting', 'married', '.'],\n",
       " ['couple', 'of', 'newbies', '?'],\n",
       " ['a', 'couple', 'of', 'gunslingers', '.']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enlines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['一', '对', '二', '胜', '。'],\n",
       " ['一', '对', '什', '么', '？'],\n",
       " ['一', '对', '太', '太', '？'],\n",
       " ['一', '对', '夫', '妇', '。'],\n",
       " ['一', '对', '夫', '妻', '。'],\n",
       " ['一', '对', '好', '人', '。'],\n",
       " ['一', '对', '情', '人', '。'],\n",
       " ['一', '对', '新', '人', '。'],\n",
       " ['一', '对', '新', '手', '？'],\n",
       " ['一', '对', '枪', '手', '。']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#中文这里 用每个字 而不是单词\n",
    "chlines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103912, 103912)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chlines),len(enlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "enwords = []\n",
    "chwords = []\n",
    "for sent in enlines:\n",
    "    for enword in sent:\n",
    "        enwords.append(enword)\n",
    "        \n",
    "for sent in chlines:\n",
    "    for chword in sent:\n",
    "        chwords.append(chword)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 49461),\n",
       " ('?', 14511),\n",
       " ('the', 10472),\n",
       " ('i', 10013),\n",
       " (',', 9489),\n",
       " ('!', 8807),\n",
       " ('you', 7493),\n",
       " ('a', 6860),\n",
       " (\"'\", 5555),\n",
       " ('it', 5524)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(enwords).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('。', 44900),\n",
       " ('我', 15087),\n",
       " ('？', 14454),\n",
       " ('你', 9087),\n",
       " ('！', 8915),\n",
       " ('了', 8663),\n",
       " ('的', 8053),\n",
       " ('，', 7291),\n",
       " ('一', 6091),\n",
       " ('是', 5946)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(chwords).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21739, 4054)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(enwords)),len(set(chwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch2ind = {}\n",
    "ind2ch = {}\n",
    "en2ind = {}\n",
    "ind2en = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "specialchars = ['<eos>','<start>','<end>','<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addchar(what2ind,ind2what,char):\n",
    "    if char in what2ind:\n",
    "        return \n",
    "    ind2what[len(what2ind)] = char\n",
    "    what2ind[char] = len(what2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<eos>': 0, '<start>': 1, '<end>': 2, '<unk>': 3}\n",
      "{0: '<eos>', 1: '<start>', 2: '<end>', 3: '<unk>'}\n"
     ]
    }
   ],
   "source": [
    "for one in specialchars:\n",
    "    addchar(ch2ind,ind2ch,one)\n",
    "    addchar(en2ind,ind2en,one)\n",
    "print(ch2ind)\n",
    "print(ind2ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只对 enwords 和 chwords 出现频率最高的前10000单词 构建 <word, index> <index, word>映射 词典\n",
    "for word,_ in Counter(enwords).most_common(10000):\n",
    "    addchar(en2ind,ind2en,word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word,_ in Counter(chwords).most_common(10000):\n",
    "    addchar(ch2ind,ind2ch,word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10004, 4058)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en2ind),len(ch2ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ？？？对应PPT 第七页“seq2seq 机器器翻译训练”图中右半边decoder图中的y in , y out 难道都是target y1, target y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_x_in = []\n",
    "# dat_y_in， dat_y_out 对应PPT 第七页“seq2seq 机器器翻译训练”图中右半边decoder图中的y in , y out （target y1）\n",
    "dat_y_in = []\n",
    "dat_y_out = []\n",
    "\n",
    "dat_x_len = []\n",
    "dat_y_len = []\n",
    "\n",
    "# ensent 每一句英语句子\n",
    "for ensent in enlines:\n",
    "    # 对英语句子里,每个单词, 取出其在词典里对应的index, 然后放入 indsent. indsent里面都是数字index\n",
    "    indsent = [en2ind.get(i,en2ind['<unk>']) for i in ensent]\n",
    "    indsent.append(en2ind['<eos>'])\n",
    "    # 以句子为单位，将当前句子里每个词装换成index 结果加上eos特殊符号的index(i.e. 0)，放入dat_x_in数组\n",
    "    # 数组dat_x_in 里每一个元素大概这样： [3, 6, 2730, 4, 0]\n",
    "    dat_x_in.append(indsent)\n",
    "    dat_x_len.append(len(indsent))\n",
    "    \n",
    "for chsent in chlines:\n",
    "    indsent = [ch2ind.get(i,ch2ind['<unk>']) for i in chsent]\n",
    "    #indsent.append(ch2ind['<eos>'])\n",
    "    # dat_y_in里每个元素大概这样： [1, 12, 63, 582, 901, 4] 1 就是start的index\n",
    "    dat_y_in.append([ch2ind['<start>']] + indsent)\n",
    "    # dat_y_out里每个元素大概这样： [12, 63, 582, 901, 4, 2] 2 就是end的index\n",
    "    dat_y_out.append(indsent + [ch2ind['<end>']])\n",
    "    # 相比较 dat_x_len 里 每个句子结尾只加了一个eos特殊符号，这里每个句子前后各加了start end\n",
    "    dat_y_len.append(len(indsent) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['couple', '.', '<eos>']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ind2en[i] for i in dat_x_in[4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start>', '一', '对', '夫', '妻', '。']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ind2ch[i] for i in dat_y_in[4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['一', '对', '夫', '妻', '。', '<end>']"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ind2ch[i] for i in dat_y_out[4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 6)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_x_len[4],dat_y_len[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103912, 103912, 103912, 103912, 103912)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dat_x_in),\\\n",
    "len(dat_y_in),\\\n",
    "len(dat_y_out),\\\n",
    "len(dat_x_len),\\\n",
    "len(dat_y_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.layers import core as layers_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn\n",
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto(log_device_placement=True,allow_soft_placement = True)\n",
    "config.gpu_options.allow_growth = True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "session = tf.Session(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#相当于把每个单词对应成512维的向量\n",
    "embedding_size = 512\n",
    "# RNN->S1->RNN->S2...这里的512就是就是S1的大小，也就是hidden unit memory 的大小\n",
    "# 512个float -> 512 * 32 bit 512 * 4字节\n",
    "#？？？ LSTM 中 最后输出 的那些分支\n",
    "# 老师解释：在LSTM里就是 相当于Ct(长期记忆)和Ht（短期记忆）都是num_unit个浮点数， 也就是memory size\n",
    "#在标准的RNN里就是是S1, S2 等这些转态的memeory size. \n",
    "num_units = 512\n",
    "# \n",
    "batch_size = 128\n",
    "#这里代码里用了1层的双向RNN，所以layer 是2。 对应课件PPT Week12.Session1.RNN-NMT.pdf第20页双向RNN图比较好理解\n",
    "layer_number = 2\n",
    "# ？？？梯度裁剪 PPT里有提到， 是对应哪个变量 是裁剪阀值c吗？\n",
    "max_grad = 1.0\n",
    "dropout = 0.2\n",
    "# 英语 翻译成 中文，这里是输入 英语词汇表的大小\n",
    "src_vocab_size = len(en2ind)\n",
    "# 英语 翻译成 中文，这里是输出 中文词汇表的大小\n",
    "target_vocat_size = len(ch2ind)\n",
    "# sentlength 上面定义过了是5\n",
    "seq_max_len = sentlength + 1\n",
    "# 老师课上提到是解码的时候最长可以输出多长，对应课件PPT Week12.Session1.RNN-NMT.pdf 7页 seq2seq 机器翻译训练\n",
    "# 右上角的<end> end 是一种结束解码的方式，这里也可以用maximum_iterations, 代码里已经用了<end>，这里这个变量可以防止句子过长,也就是解码序列超过这个值，后面的就不处理了\n",
    "maximum_iterations = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto(log_device_placement=True,allow_soft_placement = True)\n",
    "config.gpu_options.allow_growth = True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "with tf.device('/gpu:1'):\n",
    "    # all of the LSTM's parameters with the uniform distribution between -0.08 and 0.08\n",
    "    initializer = tf.random_uniform_initializer(-0.08, 0.08)\n",
    "    tf.get_variable_scope().set_initializer(initializer)\n",
    "    \n",
    "    # ？？？dat_x_in 是二维的，batch_size, time_step (每个词) batch_size 就是多少句话，time_step就是多少个每句话里多少单词？\n",
    "    x = tf.placeholder(\"int32\", [None, None])\n",
    "    # dat_y_out 二维的\n",
    "    y = tf.placeholder(\"int32\", [None, None])\n",
    "    # dat_y_in 二维的\n",
    "    y_in = tf.placeholder(\"int32\",[None,None])\n",
    "    \n",
    "    x_len = tf.placeholder(\"int32\",[None])\n",
    "    y_len = tf.placeholder(\"int32\",[None])\n",
    "    learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "    \n",
    "    # 对应PPT中图 比较好理解\n",
    "    # embedding\n",
    "    embedding_encoder = tf.get_variable(\n",
    "        \"embedding_encoder\", [src_vocab_size, embedding_size],dtype=tf.float32)\n",
    "    embedding_decoder = tf.get_variable(\n",
    "        \"embedding_decoder\", [target_vocat_size, embedding_size],dtype=tf.float32)\n",
    "\n",
    "    # 对应图左半边里面把X映射成embedding后的编码向量\n",
    "    encoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_encoder, x)\n",
    "     # 对应图右半边里面把y映射成embedding后的编码向量\n",
    "     # ？？？ y_in 对应PPT page 7 里面右半边图的最下方的输入，在training 的时候有输入，不过在predict的时候没有输入\n",
    "    decoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_decoder, y_in)\n",
    "    \n",
    "    # ？？？encoder ...这个项目里没有用到，PPT 附录里GNMT 论文里有用到\n",
    "    num_bi_layers = int(layer_number / 2)\n",
    "    \n",
    "    # Build RNN cell\n",
    "    encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "\n",
    "    # Run Dynamic RNN 左半边的RNN\n",
    "    #   encoder_outputs: [max_time, batch_size, num_units]\n",
    "    #   encoder_state: [batch_size, num_units]\n",
    "    # batch_size就是每个句子， time_step: 就是每个词\n",
    "    # 譬如两句话  我吃米。 米好吃。 可以把两句话第一个词 （我， 米）同时送进去RNN训练\n",
    "    # 左半边的训练\n",
    "    encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n",
    "        encoder_cell, encoder_emb_inp,\n",
    "        sequence_length=x_len, time_major=False,dtype=tf.float32)\n",
    "        \n",
    "    \n",
    "    # ???多少个setences?\n",
    "    batch_size_in = tf.shape(x)[0]\n",
    "    # 事先声明的dense 也就是fully connected layer， PPT 有半边图中紫色全连接的框\n",
    "    # projection_layer 也就是FC层的输出 PPT中紫红色的框\n",
    "    projection_layer = layers_core.Dense(\n",
    "        len(ch2ind), use_bias=False)\n",
    "    # Dynamic decoding\n",
    "    with tf.variable_scope(\"decode_layer\"):\n",
    "        # Build RNN cell\n",
    "        decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "        # Helper 用来接收PPT page 7, 有半边图中的y_in \n",
    "        helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "            decoder_emb_inp, y_len, time_major=False)\n",
    "        # Decoder\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            decoder_cell, helper, encoder_state,\n",
    "            output_layer=projection_layer)\n",
    "        # Dynamic decoding(training  阶段的 decode)\n",
    "        outputs, _ , __ = tf.contrib.seq2seq.dynamic_decode(decoder)\n",
    "        # 对应PPT 中softmax页面， RNN 每个时间片都输出logits\n",
    "        # RNN 在training 阶段 的每个时间片（time_step）都会输出logits， logits 也就是  预测的值\n",
    "        logits = outputs.rnn_output\n",
    "        # seq_max_len 对应PPT 15页中，batch里每个sequence的max length\n",
    "        #  tf.sequence_mask 函数就可以帮你生成PPT page 16里的mask\n",
    "        # target_weights就是PPT中提到的mask矩阵\n",
    "        target_weights = tf.sequence_mask(\n",
    "            y_len, seq_max_len, dtype=logits.dtype)\n",
    "    \n",
    "    # predicting\n",
    "    # Helper  训练 预测 共享网络参数\n",
    "    # ？？?variable_scope reuse=True  就可以在predicting 阶段共享training参数\n",
    "    with tf.variable_scope(\"decode_layer\", reuse=True):\n",
    "        # Helper\n",
    "        helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "            embedding_decoder,\n",
    "            tf.fill([batch_size_in], ch2ind['<start>']), ch2ind['<end>'])\n",
    "\n",
    "        # Decoder\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            decoder_cell, helper, encoder_state,\n",
    "            output_layer=projection_layer)\n",
    "        # Dynamic decoding\n",
    "        outputs, _ , __= tf.contrib.seq2seq.dynamic_decode(\n",
    "            decoder, maximum_iterations=maximum_iterations)\n",
    "        translations = outputs.sample_id\n",
    "        \n",
    "\n",
    "    # calculate loss\n",
    "    # sparse_softmax 可以加速运算\n",
    "    # 基于PPT16 页， 去计算training 阶段的loss， 是基于训练时候每个时间片(time_step)输出的logits计算的\n",
    "    crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    # 上面也就是那预测的值logits 和lable y去计算sparse_softmax_cross_entropy\n",
    "    #???分母中是一个batch里多少句话？？batch_size_in 为什么要除以这个？\n",
    "    # 论文里也有说呀，是把梯度除了batch_size等效于loss除batch_size\n",
    "    train_loss = (tf.reduce_sum(crossent * target_weights) / tf.cast(batch_size_in,tf.float32))\n",
    "    \n",
    "    optimizer_ori = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    trainable_params = tf.trainable_variables()\n",
    "    gradients = tf.gradients(train_loss, trainable_params)\n",
    "    # clip 就是梯度裁剪\n",
    "    clip_gradients, _ = tf.clip_by_global_norm(gradients, max_grad)\n",
    "    global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "    optimizer = optimizer_ori.apply_gradients(\n",
    "            zip(clip_gradients, trainable_params), global_step=global_step)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(train_loss)\n",
    "    #trainop = tflearn.TrainOp(loss=train_loss, optimizer=optimizer,\n",
    "    #                          metric=train_loss, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 6, 512)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(encoder_emb_inp,feed_dict={\n",
    "    x:np.asarray(dat_x_in[:1])\n",
    "}).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat_x_in = tf.keras.preprocessing.sequence.pad_sequences(dat_x_in,padding='post',value=en2ind['<eos>'])\n",
    "dat_y_in = tf.keras.preprocessing.sequence.pad_sequences(dat_y_in,padding='post',value=en2ind['<end>'])\n",
    "dat_y_out = tf.keras.preprocessing.sequence.pad_sequences(dat_y_out,padding='post',value=en2ind['<end>'])\n",
    "\n",
    "dat_x_len = np.asarray(dat_x_len)\n",
    "dat_y_len = np.asarray(dat_y_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((103912, 6), (103912, 6), (103912, 6), (103912,), (103912,))"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_x_in.shape,dat_y_in.shape,dat_y_out.shape,dat_x_len.shape,dat_y_len.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 40 batch 810 lr 7.62939453125e-06 loss 0.38680240511894226 99.90 % [=================================================>-] 103808/103912 \t used:17s eta:0 s"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "beginning_lr = 4\n",
    "for one_epoch in range(0,40):\n",
    "    index = np.asarray(list(range(len(dat_x_in))))\n",
    "    np.random.shuffle(index)\n",
    "    pb = ProgressBar(worksum=len(index))\n",
    "    pb.startjob()\n",
    "    for i in range(0,len(index),batch_size):\n",
    "        # 在分类的例子里有个data_generate去数据，这里就hard_code取数据\n",
    "        batchindex = index[i:i + batch_size]\n",
    "        \n",
    "        batch_lr = beginning_lr if one_epoch < 20 else beginning_lr * 0.5 ** (one_epoch - 20)\n",
    "        if len(batchindex) < batch_size:\n",
    "            break\n",
    "        # 如果想看一下soft mask长什么样子可以 t = session.run(target_weights,feed_dict is same)\n",
    "        _,batch_loss = session.run([optimizer,train_loss],feed_dict={\n",
    "            x:dat_x_in[batchindex],\n",
    "            y:dat_y_out[batchindex],\n",
    "            y_in:dat_y_in[batchindex],\n",
    "\n",
    "            x_len:dat_x_len[batchindex],\n",
    "            y_len:dat_y_len[batchindex],\n",
    "            learning_rate:batch_lr,\n",
    "        })\n",
    "        pb.info = \"EPOCH {} batch {} lr {} loss {}\".format(one_epoch + 1,i // batch_size,batch_lr,batch_loss)\n",
    "        pb.complete(batch_size)\n",
    "        losses.append(batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f5347d106d8>"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHIVJREFUeJzt3Xt0VfWd9/H3NzcCyB2MmKABoSDagpgqVqtVSqVoxc6o\nxXG1LLUPj1M7ra1dLVqtVVuLday1S5czeJnS8S5acarwSPFaR8GAglxUEAWC3ATCPZDL9/njbDAk\nJzn7nJxLsvm81srKvu/vjxM+2fntm7k7IiLS8eXlugAREUkPBbqISEQo0EVEIkKBLiISEQp0EZGI\nUKCLiESEAl1EJCIU6CIiEaFAFxGJiIJs7qxv375eXl6ezV2KiHR4CxYs+Mzd+yVaLquBXl5eTmVl\nZTZ3KSLS4ZnZ6jDLqctFRCQiFOgiIhGhQBcRiYis9qGLiORCbW0tVVVV1NTU5LqUVhUXF1NWVkZh\nYWFK6yvQRSTyqqqq6NatG+Xl5ZhZrsuJy93ZsmULVVVVDBw4MKVtqMtFRCKvpqaGPn36tNswBzAz\n+vTp06a/IkIFupn9xMyWmtkSM3vMzIrNbKCZzTOzlWb2hJkVpVyFiEiGtecwP6CtNSYMdDMrBX4E\nVLj7iUA+MBG4HbjL3QcD24ArE21rf11Dm4oVEZGWhe1yKQA6m1kB0AVYD5wDzAjmTwcuTLSRDzbu\nTKVGEZFImD17NkOHDmXw4MFMnTo17dtPGOjuvg74d2ANsSDfDiwAqt29LlisCiiNt76ZTTazSjPT\nLaIictiqr6/n6quvZtasWSxbtozHHnuMZcuWpXUfYbpcegETgIHA0UBXYFzYHbj7NHevcPeKlKsU\nEeng5s+fz+DBgxk0aBBFRUVMnDiRmTNnpnUfYS5b/DrwsbtvBjCzZ4DTgZ5mVhAcpZcB69JamYhI\nBtz8P0tZ9umOtG5z+NHduelbJ7S6zLp16xgwYMDB8bKyMubNm5fWOsL0oa8BRptZF4udgh0DLANe\nBi4KlpkEpPdXjYiIJCXhEbq7zzOzGcBCoA54B5gGPA88bma/CaY9mMlCRUTSIdGRdKaUlpaydu3a\ng+NVVVWUlsY99ZiyUHeKuvtNwE1NJq8CTklrNSIiEfXlL3+ZFStW8PHHH1NaWsrjjz/Oo48+mtZ9\n6NZ/EZEsKCgo4J577uHcc8+lvr6eK664ghNOSO9fCwp0EZEsGT9+POPHj8/Y9vUsFxGRiFCgi4hE\nhAJdRA4L7p7rEhJqa40KdBGJvOLiYrZs2dKuQ/3A89CLi4tT3oZOiopI5JWVlVFVVcXmzZtzXUqr\nDryxKFUKdBGJvMLCwpTfAtSRqMtFRCQiFOgiIhGhQBcRiQgFuohIRCjQRUQiQoEuIhIRCnQRkYgI\n807RoWb2bqOvHWZ2jZn1NrM5ZrYi+N4rGwWLiEh8CQPd3T9w95HuPhI4GdgD/BWYAsx19yHA3GBc\nRERyJNkulzHAR+6+GpgATA+mTwcuTGdhIiKSnGQDfSLwWDBc4u7rg+ENQEnaqhIRkaSFDnQzKwIu\nAJ5qOs9jjzCL+xgzM5tsZpVmVplylSIiklAyR+jfBBa6+8ZgfKOZ9QcIvm+Kt5K7T3P3CnevaFup\nIiLSmmQC/VI+724BeA6YFAxPAmamqygREUleqEA3s67AWOCZRpOnAmPNbAXw9WBcRERyJNTz0N19\nN9CnybQtxK56ERGRdkB3ioqIRIQCXUQkIhToIiIRoUAXEYkIBbqISEQo0EVEIkKBLiISEQp0EZGI\nUKCLiESEAl1EJCIU6CIiEaFAFxGJCAW6iEhEKNBFRCJCgS4iEhEKdBGRiAj7xqKeZjbDzN43s+Vm\ndpqZ9TazOWa2IvjeK9PFiohIy8Ieod8NzHb3YcAIYDkwBZjr7kOAucG4iIjkSMJAN7MewJnAgwDu\nvt/dq4EJwPRgsenAhZkqUkREEgtzhD4Q2Az8l5m9Y2YPBC+NLnH39cEyG4CSTBUpIiKJhQn0AmAU\ncJ+7nwTspkn3irs74PFWNrPJZlZpZpVtLVZERFoWJtCrgCp3nxeMzyAW8BvNrD9A8H1TvJXdfZq7\nV7h7RToKFhGR+BIGurtvANaa2dBg0hhgGfAcMCmYNgmYmZEKRUQklIKQy/0b8IiZFQGrgMuJ/TJ4\n0syuBFYDl2SmRBERCSNUoLv7u0C8LpMx6S1HRERSpTtFRUQiQoEuIhIRCnQRkYhQoIuIRIQCXUQk\nIhToIiIRoUAXEYkIBbqISEQo0EVEIkKBLiISEQp0EZGIUKCLiESEAl1EJCIU6CIiEaFAFxGJiFDP\nQzezT4CdQD1Q5+4VZtYbeAIoBz4BLnH3bZkpU0REEknmCP1sdx/Z6N2gU4C57j4EmEuTF0eLiEh2\ntaXLZQIwPRieDlzY9nJERCRVYQPdgRfNbIGZTQ6mlbj7+mB4A1CS9upERCS0sC+JPsPd15nZkcAc\nM3u/8Ux3dzPzeCsGvwAmAxQdNbhNxYqISMtCHaG7+7rg+ybgr8ApwEYz6w8QfN/UwrrT3L2iUd+7\niIhkQMJAN7OuZtbtwDDwDWAJ8BwwKVhsEjAzU0WKiEhiYbpcSoC/mtmB5R9199lm9jbwpJldCawG\nLslcmSIikkjCQHf3VcCIONO3AGMyUZSIiCRPd4qKiESEAl1EJCIU6CIiEaFAFxGJCAW6iEhEKNBF\nRCJCgS4iEhEKdBGRiFCgi4hEhAJdRCQiFOgiIhGhQBcRiQgFuohIRCjQRUQiQoEuIhIRCnQRkYgI\nHehmlm9m75jZ34LxgWY2z8xWmtkTZlaUuTJFRCSRZI7QfwwsbzR+O3CXuw8GtgFXprMwERFJTqhA\nN7My4DzggWDcgHOAGcEi04ELM1GgiIiEE/YI/Y/Az4GGYLwPUO3udcF4FVCa5tpERCQJCQPdzM4H\nNrn7glR2YGaTzazSzCpTWV9ERMIpCLHM6cAFZjYeKAa6A3cDPc2sIDhKLwPWxVvZ3acB0wA69R/i\naalaRESaSXiE7u7XuXuZu5cDE4GX3P0y4GXgomCxScDMjFUpIiIJteU69F8APzWzlcT61B9MT0ki\nIpKKMF0uB7n7K8ArwfAq4JT0lyQiIqnQnaIiIhGR9UB/9cPN2d6liMhhIeuB/sGGHdnepYjIYSHr\nge66cFFEJCOyHugNCnQRkYzIeqBv3FGT7V2KiBwWctDlokN0EZFMyHqg1yvQRUQyIvuBrk50EZGM\nUKCLiERE1gO9ToEuIpIRWQ/0F5duzPYuRUQOC1kP9F376hIvJCIiSdPDuUREIkKBLiISEQp0EZGI\nCPOS6GIzm29mi8xsqZndHEwfaGbzzGylmT1hZkWZL1dERFoS5gh9H3COu48ARgLjzGw0cDtwl7sP\nBrYBV2auTBERSSTMS6Ld3XcFo4XBlwPnADOC6dOBCzNSoYiIhBKqD93M8s3sXWATMAf4CKh29wPX\nIFYBpS2sO9nMKs2sMh0Fi4hIfKEC3d3r3X0kUEbsxdDDwu7A3ae5e4W7V6RYo4iIhJDUVS7uXg28\nDJwG9DSzgmBWGbAuzbWJiEgSwlzl0s/MegbDnYGxwHJiwX5RsNgkYGamihQRkcTCHKH3B142s8XA\n28Acd/8b8Avgp2a2EugDPBh2p598tjuVWkVEpBUFiRZw98XASXGmryLWn560y//8Ni//7GuprCoi\nIi3IyZ2iH+sIXUQk7XTrv4hIROQs0HfrMboiImmVs0B/fcXmXO1aRCSSshroJ5b2ODh81cML2b6n\nNpu7FxGJtKwGujUZf2fttmzuXkQk0nJ6UrReL4wWEUmbnAb6vz68kEVrq3NZgohIZGQ90E8b1Ofg\n8P76Bibc+0a2SxARiaSsB/rEUwZke5ciIoeFrAf6qQP7NJv20eZdcZYUEZFkZD3Qj+pR3GzamDtf\nzXYZIiKRk5OTot2Kmz8TrEFXvIiItElOAn3utWc1m/bo/DU5qEREJDpyEuhHdmve7XLDs0tYXKVL\nGEVEUhXmjUUDzOxlM1tmZkvN7MfB9N5mNsfMVgTfeyWz4/O+2L/ZtAvu0SWMIiKpCnOEXgdc6+7D\ngdHA1WY2HJgCzHX3IcDcYDy0ey8bFXe6u/rSRURSkTDQ3X29uy8MhncSe59oKTABmB4sNh24MB0F\n/fdbq9OxGRGRw05SfehmVk7sdXTzgBJ3Xx/M2gCUJLvz2dd8tdm0X81cSvmU59lZoycxiogkI3Sg\nm9kRwNPANe6+o/E8j/WTxO0rMbPJZlZpZpWbNx/6DPRhR3VvcX8vLt0YtjQRESFkoJtZIbEwf8Td\nnwkmbzSz/sH8/sCmeOu6+zR3r3D3in79+jWbf9HJZXH3ee1Ti7jh2ffClCciIoS7ysWAB4Hl7v6H\nRrOeAyYFw5OAmakU8Pt//lKL8x5+a41uOBIRCSnMEfrpwHeBc8zs3eBrPDAVGGtmK4CvB+PJF5Bn\nDOrbtcX5g65/gZ8+8S7rqveycI1eiCEi0hLL5mWCFRUVXllZ2Wy6uzPwuhdCbeMvV5zCmV9o3nUj\nIhJVZrbA3SsSLZfTF1wcEOvVCecfKz/LYCUiIh1Xuwh0gE+mnhdquQdeX5XhSkREOqZ2E+gAPxoz\nJOEyOkcqIhJfuwr0q88+jtMHN38BRlOjb5vLTTOXUFffkIWqREQ6hnYV6J0K8nnk+6MZ1K/lq14A\nNuyoYfqbqxn8y1l8Wr2XZZ/uYF9dfZaqFBFpn9rFVS5N1Tc4a7bu4ex/fyWp7Zf27MwbU85JsToR\nkfapQ13l0lR+njGwb1eK8pMrb1313gxVJCLS/rXLQD9g7rVn8YdLRiS1zpSnF2eoGhGR9q1dB/qA\n3l34p1FlvHXdmNDrPP722gxWJCLSfrXrQD/gqB7NX1knIiKH6hCBDlCQF/5u0gdeX8XxN86mfMrz\nPDJvNcffOJuqbXtYu3VPBisUEcmtdnmVSzxrtuzhzDtebnMNF59cxh0XJ9cvLyKSSx36Kpd4junT\nJS3beWpBVVq2IyLS3nSYQG9sQO/OXHrKMQzo3bnN22pocN7RY3lFJAIKcl1AMp7/0RlMnfU+d148\ngiO7x06ULli9lX++782ktlNTW09xYT6zl6xn2fqd/GnuCgBmXHUa/bp14uienSlM8hp4EZFc6zB9\n6Ikk80x1M7h1wonc8OySuPMvP72cm751QjrLExFJWdr60M3sITPbZGZLGk3rbWZzzGxF8L1XWwtu\nq2Seqe5Oi2EO8OqHm1ucJyLSXoXpV/gzMK7JtCnAXHcfAswNxiNj1ebduS5BRCRpCQPd3V8DtjaZ\nPAGYHgxPBy5Mc10iIpKkVE+Klrj7+mB4A1DS0oJmNhmYDHDMMcekuLtwlt1yLgtWb+OITgU8s3Ad\n5xx/JACnDuzNorXbufT+t5La3o6aWi6673/5cOMuenYp5PHJoxn3x9c574v9ufeyUZlogohIykKd\nFDWzcuBv7n5iMF7t7j0bzd/m7gn70TN5UjSM8inPp21bYV+ZJyLSVpm+sWijmfUPdtQf2JTidiJn\n+55aqrbpEQMikn2pBvpzwKRgeBIwMz3ldGxbd+9nxC0vcsbtbX9EgYhIshL2oZvZY8DXgL5mVgXc\nBEwFnjSzK4HVwCWZLDJdfnTOYP700sq0bGvLrn3sr2+gS2EB/3TfG3ykK2NEJMcic2NRWDW19fzw\n0YX8ffkmXvnZ1yjv+/n7S/fur+f4X81Oy37euXEsvboWpWVbInJ4C9uH3qFu/U+H4sJ87v9eBe6Q\n1+SRvEncm5TQ4nXbOesL/QD43azlDDuqG5t27ON3s94HYPkt4+hclJ++HYrIYe+wC3SI3VUaL7wb\n0vjXytzlG5n00Hx+MW4Y//nqqmbzp85azs0TTkzb/kRE9ASqRjoXxj9ivvz0cp69+vSkLlX8y5ur\nAbh99vtx508P5jfV0ODU1NaH3o+IyAGH5RF6S8yMR75/Kpc9MO/gtFW3jW/WNZMJDQ3ODx5ZyOyl\nGwB4/9ZxFLfwC0ZEJB4FehOnD+7L+7eOo77B6dops/882/fUUpBvPFm5licrq1i+fsfBecNunK2b\nl0QkKQr0OJI9Mj7vS/15fvF6nr36dC68943Q64245cVQy7324WZKuhczsG9Xfvv8MnbW1LF1z37+\nfPkpSdUpItGmQE/RZacew3Xjj2d99V6GlHTj3n/JzH4+27WP7z00P+68Jeu2c2Jpj8zsWEQ6HAV6\nkj6Zeh4NDX6wX31ISbdD5ncuzGdvmk5qzlhQxc+eWtTi/N8+v5zHJo8+ZNrST7dz3p/+AagfXuRw\no0BPQWsnSWdf81XOuuOVZtPnXT+GZZ/u4OxhR4Z+SFhrYQ6wp7aeTTtqmPbaKh74x8fN5jfth3d3\nNu/ch5nx4rINHNu7K2cM6RuqFhFp/xToaXZsn67c/70K7nlpBY9NHk2XogLcHTOjJHgParosWlvN\nKbfNTbjcio07GXvXa3HntXTitaa2nlWbdzP86O5tqlFEskeBngFjh5cwdvjnj4hP9Hq8scNLGHVM\nL0YM6MFXjuub1sf8btpZ02KYN9bQ4Lz0/iZ+/vRitu7ef3D63GvP4rh+Rxyy7Lbd++lclK/uHJF2\nRoGeQ/ddNoqvHNeXHl0KQ6/z2P8ZzcPzVvP84vWJFwZO+W3rR/CJfnmMufNVPpl6Hq+v2MzC1dV0\nKsxj6qzPb5a66zsj+PZJZYess7+ugc279rF1136O7llMnyM6hapVRNpGgZ4DyV5ffvfEkUwYWXpw\nvDDfQgd6Ogy9YRb76hrizvvJE4sY/8X+/G3Rev4w50PWVe9ttsxHt40nP894Y+VnnHh0Dz7dvpcr\n/vw2R/Uo5p011XEfZLZ1935G3TqHMcOO5DffPpH+PTpnpG0iUXLYPW2xI3ji7TX84un3mHHVaYwc\n0JOC/EOf0LBobTUT4lzvfsdFX+LiigFAet/O1FbfPPEoZi3Z0Ooyn0w9j9tnv899r3zU4vya2no+\n2LCTqx5ewPrtNYfM//h34zEzNu2s4cHXP+asof34l/s/v+P3+vHDmHzmcUDs5PCarXsYc+er1DXE\nfv6/fVIpd31n5MHlt+zax9pte6natofrn3mPUwf14b7LRjX7LESyIezTFhXoHZC7M/C6Fw6O3zLh\nBL53Wvkhy8QL9Lu+M4Kxw4+ivsEZcXO4m5oOJ7Ov+SrffXA+m3fua3EZ3b0ruZCVx+ea2TjgbiAf\neMDdp7ZlexKOmSUMlm8ML2Hhmmq+clwffvWt4fQN0Y997dgvcGJpD742tB/Ve2o56dY56Sq5Qxj3\nx9cTLnP331ewp7aOjdtryM/L4+mFVQAM7NuVJ/7vaI7s9vmVTHX1DTqil6xK+QjdzPKBD4GxQBXw\nNnCpuy9raR0dobcfDQ3ObS8s59/GDKFbp4Jm19bXNzjHXR/7K2DSacfy83HDKMzPoyDPDi4b76+A\ns4f246djh3LC0d357QvLeTDO9fEAnQryeOqq07jgnvCPSoiC+b8cQ74ZPbsUUbVtDz06F9KlqIAt\nu/cdPE+wa18dXQrzycsztu3ez97aeo7u2ZmdNbWYGUd0KmBd9V521dQx9KhuCfYY+4uutt4pKmif\nv1xqauuprW+gW3EhtfUNGGTkF6G7U9fgFGZg29v31HJEcQH5GXqQXzaO0E8BVrr7qmCHjwMTgBYD\nXdqPvDzjhvOHtzg/Py/xXwEf/GYcu2rqOKK4gKL8vGaXZ954/nC6FxfSuSiPk4/tRXmfrs2ueOnZ\npZDqPbUAPPr9U9m1r44xx5cc/I9x08wlBx81PGbYkdx5yQgK8vPIM+hSFAu206e+dMg2v1MxgIsr\nyqgo7w3E/8XTp2sRs378Vbp3LmTYjc3fUjXkyCP4xgklHN2zM0OO7MYl//lmq/8WYSW66kikLdpy\nhH4RMM7dvx+Mfxc41d1/2NI6OkKXXNm+t5YenVu+PHR/XQNbd+/nqB7xb/66dNpbvLlqS6bKE2nV\n6tvPbx+voDOzycBkgGOOOSbTuxOJq7UwBygqyGsxzIFmz8yJ538WfcrW3fsp6V7Mvrp6tu3ejwMX\njDiatz/ZylUPL0y2bJGktOUI/TTg1+5+bjB+HYC7/66ldXSELiKSvLB96G05O/A2MMTMBppZETAR\neK4N2xMRkTZIucvF3evM7IfA/yN22eJD7r40bZWJiEhS2tSH7u4vAC8kXFBERDKufV6YKiIiSVOg\ni4hEhAJdRCQiFOgiIhGhQBcRiYisPj7XzHYCH2Rth5nVF/gs10WkkdrTfkWpLRCt9mSrLce6e79E\nC2X7jUUfhLnbqSMws8qotAXUnvYsSm2BaLWnvbVFXS4iIhGhQBcRiYhsB/q0LO8vk6LUFlB72rMo\ntQWi1Z521ZasnhQVEZHMUZeLiEhEZCXQzWycmX1gZivNbEo29pkqM/vEzN4zs3fNrDKY1tvM5pjZ\niuB7r2C6mdmfgnYtNrNRjbYzKVh+hZlNylLtD5nZJjNb0mha2mo3s5ODf5uVwbqZeYFi6+35tZmt\nCz6fd81sfKN51wW1fWBm5zaaHvfnL3j087xg+hPBY6Az1ZYBZvaymS0zs6Vm9uNgeof8fFppT0f9\nfIrNbL6ZLQrac3NrNZhZp2B8ZTC/PNV2ppW7Z/SL2KN1PwIGAUXAImB4pvfbhno/Afo2mfZ7YEow\nPAW4PRgeD8wCDBgNzAum9wZWBd97BcO9slD7mcAoYEkmagfmB8tasO43c9CeXwM/i7Ps8OBnqxMw\nMPiZy2/t5w94EpgYDP8H8K8ZbEt/YFQw3I3YC9aHd9TPp5X2dNTPx4AjguFCYF7wbxm3BuAHwH8E\nwxOBJ1JtZzq/snGEfvBl0u6+HzjwMumOZAIwPRieDlzYaPpfPOYtoKeZ9QfOBea4+1Z33wbMAcZl\nukh3fw3Ymonag3nd3f0tj/3k/qXRtrLZnpZMAB53933u/jGwktjPXtyfv+Do9RxgRrB+43+btHP3\n9e6+MBjeCSwHSumgn08r7WlJe/983N13BaOFwZe3UkPjz20GMCaoOal2prsd2Qj0UmBto/EqWv/g\nc82BF81sgcXehwpQ4u7rg+ENQEkw3FLb2lOb01V7aTDcdHou/DDohnjoQBcFybenD1Dt7nVNpmdc\n8Of5ScSOAjv859OkPdBBPx8zyzezd4FNxH5RftRKDQfrDuZvD2rOaSbopGhzZ7j7KOCbwNVmdmbj\nmcHRT4e8NKgj197IfcBxwEhgPXBnbstJjpkdATwNXOPuOxrP64ifT5z2dNjPx93r3X0kUEbsiHpY\njktKWjYCfR0woNF4WTCtXXL3dcH3TcBfiX2wG4M/aQm+bwoWb6lt7anN6ap9XTDcdHpWufvG4D9e\nA3A/sc8Hkm/PFmLdGAVNpmeMmRUSC79H3P2ZYHKH/Xzitacjfz4HuHs18DJwWis1HKw7mN8jqDm3\nmZDuTvmmX8SeF7OK2AmCAycDTsj0flOstSvQrdHw/xLr+76DQ09c/T4YPo9DT1zND6b3Bj4mdtKq\nVzDcO0ttKOfQk4hpq53mJ93G56A9/RsN/4RYfyXACRx6MmoVsRNRLf78AU9x6AmvH2SwHUasX/uP\nTaZ3yM+nlfZ01M+nH9AzGO4MvA6c31INwNUcelL0yVTbmdZ2ZOofqMk/1nhiZ8E/An6ZjX2mWOeg\n4B96EbD0QK3E+sbmAiuAvzf6D2TAvUG73gMqGm3rCmInRFYCl2ep/seI/ZlbS6yP7sp01g5UAEuC\nde4huDEty+3576DexcBzTQLkl0FtH9DoCo+Wfv6Cz3t+0M6ngE4ZbMsZxLpTFgPvBl/jO+rn00p7\nOurn8yXgnaDuJcCvWqsBKA7GVwbzB6XaznR+6U5REZGI0ElREZGIUKCLiESEAl1EJCIU6CIiEaFA\nFxGJCAW6iEhEKNBFRCJCgS4iEhH/H6MO7fRzJu4BAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5347d06a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "pd.DataFrame(losses).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def translate(sent):\n",
    "    senttoken = [en2ind[i.lower()] for i in sent.split()]\n",
    "    senttoken.append(en2ind['<eos>'])\n",
    "    inputx = np.asarray([senttoken])\n",
    "    inputx_len = np.asarray([len(senttoken)])\n",
    "    print(inputx,inputx_len)\n",
    "    batch_translations = session.run(translations,feed_dict={\n",
    "            x:inputx,\n",
    "            x_len:inputx_len,\n",
    "        })[0]\n",
    "    return ''.join([ind2ch[i] for i in batch_translations])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_sentence = \"i love shopping .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   7   84 1249    4    0]] [5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'我爱购物。<end>'"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(source_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/seq2seq_model'"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(session,'models/seq2seq_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 meta meta 53903364  7月  5 20:45 models/seq2seq_model.data-00000-of-00001\r\n"
     ]
    }
   ],
   "source": [
    "! ls -l 'models/seq2seq_model.data-00000-of-00001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
